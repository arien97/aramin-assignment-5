{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the KNN class\n",
    "class KNN:\n",
    "    def __init__(self, k=5, distance_metric='euclidean', p=2):\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "        self.p = p\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = np.array(y)  # Convert y to a NumPy array\n",
    "        self.classes_ = np.unique(y)  # Add this line to set the classes_ attribute\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            distances = self.compute_distances(x)\n",
    "            k_nearest = self.get_k_nearest(distances)\n",
    "            predictions.append(self.get_majority_class(k_nearest))\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def compute_distances(self, x):\n",
    "        if self.distance_metric == 'euclidean':\n",
    "            return np.linalg.norm(self.X_train - x, axis=1)\n",
    "        elif self.distance_metric == 'manhattan':\n",
    "            return np.sum(np.abs(self.X_train - x), axis=1)\n",
    "        elif self.distance_metric == 'minkowski':\n",
    "            return np.sum(np.abs(self.X_train - x) ** self.p, axis=1) ** (1 / self.p)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown distance metric: {self.distance_metric}\")\n",
    "\n",
    "    def get_k_nearest(self, distances):\n",
    "        return np.argsort(distances)[:self.k]\n",
    "\n",
    "    def get_majority_class(self, k_nearest):\n",
    "        k_nearest_labels = np.array(self.y_train[k_nearest], dtype=int)\n",
    "        return np.bincount(k_nearest_labels).argmax()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        probas = []\n",
    "        for x in X:\n",
    "            distances = self.compute_distances(x)\n",
    "            k_nearest = self.get_k_nearest(distances)\n",
    "            k_nearest_labels = self.y_train[k_nearest]\n",
    "            class1_prob = np.mean(k_nearest_labels == 1)\n",
    "            probas.append([1 - class1_prob, class1_prob])  # Probabilities for [class 0, class 1]\n",
    "        return np.array(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_file, test_file):\n",
    "    train_df = pd.read_csv(train_file)\n",
    "    test_df = pd.read_csv(test_file)\n",
    "    \n",
    "    # Target encoding for 'Geography' and 'Gender'\n",
    "    target_cols = ['Geography', 'Gender']\n",
    "    for col in target_cols:\n",
    "        train_df[col] = train_df[col].astype('category').cat.codes\n",
    "        test_df[col] = test_df[col].astype('category').cat.codes\n",
    "\n",
    "    # Create interaction features as an example\n",
    "    train_df['Credit_Age'] = train_df['CreditScore'] / (train_df['Age'] + 1)\n",
    "    test_df['Credit_Age'] = test_df['CreditScore'] / (test_df['Age'] + 1)\n",
    "\n",
    "    X = train_df.drop(columns=['id', 'CustomerId', 'Surname', 'Exited'])\n",
    "    y = train_df['Exited']\n",
    "    X_test = test_df.drop(columns=['id', 'CustomerId', 'Surname'])\n",
    "\n",
    "    return X, y, X_test\n",
    "\n",
    "def custom_kfold(X, y, n_splits=3):\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    fold_sizes = np.full(n_splits, X.shape[0] // n_splits, dtype=int)\n",
    "    fold_sizes[:X.shape[0] % n_splits] += 1\n",
    "    \n",
    "    current = 0\n",
    "    folds = []\n",
    "    \n",
    "    for fold_size in fold_sizes:\n",
    "        start, stop = current, current + fold_size\n",
    "        val_indices = indices[start:stop]\n",
    "        train_indices = np.concatenate([indices[:start], indices[stop:]])\n",
    "        folds.append((train_indices, val_indices))\n",
    "        current = stop\n",
    "    \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(X, y, knn, n_splits=3):\n",
    "    # Convert to NumPy arrays to avoid issues with indexing\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    folds = custom_kfold(X, y, n_splits)\n",
    "    auc_scores = []\n",
    "    \n",
    "    for train_indices, val_indices in folds:\n",
    "        X_train, X_val = X[train_indices], X[val_indices]\n",
    "        y_train, y_val = y[train_indices], y[val_indices]\n",
    "        \n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred_proba = knn.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        if len(np.unique(y_val)) > 1:\n",
    "            auc_score = custom_roc_auc_score(y_val, y_pred_proba)\n",
    "            auc_scores.append(auc_score)\n",
    "        else:\n",
    "            print(\"Only one class present in y_true. Skipping this fold.\")\n",
    "    \n",
    "    return auc_scores\n",
    "\n",
    "def custom_roc_auc_score(y_true, y_score):\n",
    "    pos_label = 1\n",
    "    neg_label = 0\n",
    "    \n",
    "    desc_score_indices = np.argsort(y_score)[::-1]\n",
    "    y_true_sorted_by_score = y_true[desc_score_indices]\n",
    "    \n",
    "    distinct_value_indices = np.where(np.diff(y_score[desc_score_indices]))[0]\n",
    "    \n",
    "    threshold_idxs = np.r_[distinct_value_indices, y_true_sorted_by_score.size - 1]\n",
    "    \n",
    "    tps = np.cumsum(y_true_sorted_by_score == pos_label)[threshold_idxs]\n",
    "    \n",
    "    fps = 1 + threshold_idxs - tps\n",
    "    \n",
    "    tps = np.r_[0, tps]\n",
    "    fps = np.r_[0, fps]\n",
    "    \n",
    "    fpr = fps / fps[-1]\n",
    "    tpr = tps / tps[-1]\n",
    "    \n",
    "    return np.trapz(tpr, fpr)\n",
    "\n",
    "def standardize_data(X):\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std_dev = np.std(X, axis=0)\n",
    "    return (X - mean) / std_dev\n",
    "\n",
    "def add_polynomial_features(X, degree=2):\n",
    "    poly_features = [X]\n",
    "    for d in range(2, degree + 1):\n",
    "        poly_features.append(X ** d)\n",
    "    return np.concatenate(poly_features, axis=1)\n",
    "\n",
    "def apply_pca(X, n_components=24):\n",
    "    mean_X = np.mean(X, axis=0)\n",
    "    centered_X = X - mean_X\n",
    "    covariance_matrix = np.cov(centered_X.T)\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    top_eigenvectors = eigenvectors[:, sorted_indices[:n_components]]\n",
    "    return centered_X @ top_eigenvectors\n",
    "\n",
    "def smote_enn(X, y):\n",
    "    def smote(X_minority):\n",
    "        n_samples, n_features = X_minority.shape\n",
    "        synthetic_samples = []\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            sample_i = X_minority[i]\n",
    "            neighbors_indices = np.random.choice(n_samples, size=5, replace=False)\n",
    "            neighbor_sample = X_minority[neighbors_indices[np.random.randint(5)]]\n",
    "            \n",
    "            diff = neighbor_sample - sample_i\n",
    "            synthetic_sample = sample_i + diff * np.random.rand()\n",
    "            \n",
    "            synthetic_samples.append(synthetic_sample)\n",
    "        \n",
    "        return np.array(synthetic_samples)\n",
    "\n",
    "    def enn(X_resampled, y_resampled):\n",
    "        def nearest_neighbors(X, point_idx, n_neighbors=3):\n",
    "            distances = np.linalg.norm(X - X[point_idx], axis=1)\n",
    "            neighbors_indices = np.argsort(distances)[1:n_neighbors+1]\n",
    "            return neighbors_indices\n",
    "        \n",
    "        indices_to_remove = []\n",
    "        \n",
    "        for i in range(len(X_resampled)):\n",
    "            neighbors_indices = nearest_neighbors(X_resampled, i)\n",
    "            neighbors_labels = y_resampled[neighbors_indices]\n",
    "            \n",
    "            if len(np.unique(neighbors_labels)) > 1:\n",
    "                indices_to_remove.append(i)\n",
    "        \n",
    "        mask_to_keep = ~np.isin(np.arange(len(X_resampled)), indices_to_remove)\n",
    "        \n",
    "        return X_resampled[mask_to_keep], y_resampled[mask_to_keep]\n",
    "\n",
    "    minority_class_label = min(np.unique(y), key=lambda x: np.sum(y == x))\n",
    "    \n",
    "    X_minority_class_samples = X[y == minority_class_label]\n",
    "    \n",
    "    synthetic_samples_minority_class = smote(X_minority_class_samples)\n",
    "    \n",
    "    X_resampled = np.vstack((X, synthetic_samples_minority_class))\n",
    "    y_resampled = np.hstack((y, np.full(len(synthetic_samples_minority_class), minority_class_label)))\n",
    "    \n",
    "    X_resampled_enn, y_resampled_enn = enn(X_resampled, y_resampled)\n",
    "    \n",
    "    return X_resampled_enn, y_resampled_enn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m X_test_poly \u001b[38;5;241m=\u001b[39m add_polynomial_features(X_test_scaled, degree\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Step 3: Apply SMOTEENN for class balancing (optional, for training data only)\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m X_balanced, y_balanced \u001b[38;5;241m=\u001b[39m \u001b[43msmote_enn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_poly\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Step 4: PCA with more components\u001b[39;00m\n\u001b[0;32m     16\u001b[0m X_reduced \u001b[38;5;241m=\u001b[39m apply_pca(X_balanced, n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 112\u001b[0m, in \u001b[0;36msmote_enn\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m    109\u001b[0m X_resampled \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack((X, synthetic_samples_minority_class))\n\u001b[0;32m    110\u001b[0m y_resampled \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((y, np\u001b[38;5;241m.\u001b[39mfull(\u001b[38;5;28mlen\u001b[39m(synthetic_samples_minority_class), minority_class_label)))\n\u001b[1;32m--> 112\u001b[0m X_resampled_enn, y_resampled_enn \u001b[38;5;241m=\u001b[39m \u001b[43menn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_resampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_resampled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_resampled_enn, y_resampled_enn\n",
      "Cell \u001b[1;32mIn[11], line 93\u001b[0m, in \u001b[0;36msmote_enn.<locals>.enn\u001b[1;34m(X_resampled, y_resampled)\u001b[0m\n\u001b[0;32m     90\u001b[0m indices_to_remove \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_resampled)):\n\u001b[1;32m---> 93\u001b[0m     neighbors_indices \u001b[38;5;241m=\u001b[39m \u001b[43mnearest_neighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_resampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m     neighbors_labels \u001b[38;5;241m=\u001b[39m y_resampled[neighbors_indices]\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(neighbors_labels)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "Cell \u001b[1;32mIn[11], line 86\u001b[0m, in \u001b[0;36msmote_enn.<locals>.enn.<locals>.nearest_neighbors\u001b[1;34m(X, point_idx, n_neighbors)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnearest_neighbors\u001b[39m(X, point_idx, n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m---> 86\u001b[0m     distances \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpoint_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m     neighbors_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(distances)[\u001b[38;5;241m1\u001b[39m:n_neighbors\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m neighbors_indices\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\linalg\\_linalg.py:2620\u001b[0m, in \u001b[0;36m_norm_dispatcher\u001b[1;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[0;32m   2616\u001b[0m     result \u001b[38;5;241m=\u001b[39m op(svd(y, compute_uv\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   2617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m-> 2620\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_norm_dispatcher\u001b[39m(x, \u001b[38;5;28mord\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   2621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (x,)\n\u001b[0;32m   2624\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_norm_dispatcher)\n\u001b[0;32m   2625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnorm\u001b[39m(x, \u001b[38;5;28mord\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "X, y, X_test = preprocess_data('train.csv', 'test.csv')\n",
    "\n",
    "# Step 1: Standardize the data\n",
    "X_scaled = standardize_data(X)\n",
    "X_test_scaled = standardize_data(X_test)\n",
    "\n",
    "# Step 2: Add Polynomial Features (optional)\n",
    "X_poly = add_polynomial_features(X_scaled, degree=2)\n",
    "X_test_poly = add_polynomial_features(X_test_scaled, degree=2)\n",
    "\n",
    "# Step 3: Apply SMOTEENN for class balancing (optional, for training data only)\n",
    "X_balanced, y_balanced = smote_enn(X_poly, y)\n",
    "\n",
    "# Step 4: PCA with more components\n",
    "X_reduced = apply_pca(X_balanced, n_components=24)\n",
    "X_test_reduced = apply_pca(X_test_poly, n_components=24)\n",
    "\n",
    "# Step 5: Use a more complex model (KNN in this case since we can't use other models without sklearn imports)\n",
    "knn_model = KNN(k=5, distance_metric='minkowski', p=1.5)\n",
    "\n",
    "# Step 6: Cross-validate the model\n",
    "cv_auc_scores = cross_validate(X_reduced, y_balanced, knn_model, n_splits=5)\n",
    "print(f\"Cross-Validation AUC Scores: {cv_auc_scores}\")\n",
    "print(f\"Mean Cross-Validation AUC Score: {np.mean(cv_auc_scores)}\")\n",
    "\n",
    "# Step 7: Make predictions on the test set using the best model (as probabilities)\n",
    "knn_model.fit(X_reduced, y_balanced)\n",
    "\n",
    "# Use predict_proba to get probabilities instead of binary predictions\n",
    "test_probabilities = knn_model.predict_proba(X_test_reduced)[:, 1]  # Select probability of class 1 (Exited)\n",
    "\n",
    "# Save test predictions\n",
    "pd.DataFrame({'id': pd.read_csv('test.csv')['id'], 'Exited': test_probabilities}).to_csv('submissions15.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
